# Core ML/AI libraries
torch
transformers
datasets
accelerate
peft
bitsandbytes

# DPO / RLHF training (Smart-LLaMA-DPO approach)
trl

# Ethereum and blockchain libraries
web3
py-evm
eth-abi
eth-utils
evmdasm
py-solc-x

# Data processing and analysis
numpy
pandas
scipy
scikit-learn

# Evaluation and metrics
nltk
rouge-score
sentence-transformers

# Malicious contract classifier (Explainable AI approach)
lightgbm
lime
shap

# Visualization and logging
matplotlib
seaborn
wandb
tensorboard

# Hugging Face Hub
huggingface_hub

# HTTP requests
requests

# GPU monitoring
nvidia-ml-py3

# File processing and utilities
tqdm
click
pyyaml
jsonlines

# Report generation
jinja2

# Development and testing
pytest
black
flake8
mypy

# Optional: Flash Attention 2 for faster inference
# Requires CUDA toolkit; install with: pip install flash-attn --no-build-isolation
# flash-attn

# Optional: For distributed training
# deepspeed>=0.10.0
