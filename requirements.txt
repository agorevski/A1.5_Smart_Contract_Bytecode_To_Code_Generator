# Core ML/AI libraries
torch>=2.0,<3
transformers>=4.35,<5
datasets>=2.14,<3
accelerate>=0.24,<1
peft>=0.6,<1
bitsandbytes>=0.41,<1

# DPO / RLHF training (Smart-LLaMA-DPO approach)
trl>=0.7,<1

# Ethereum and blockchain libraries
web3>=6.0,<7
py-evm>=0.10,<1
eth-abi>=4.0,<6
eth-utils>=2.0,<5
evmdasm>=0.1,<1
py-solc-x>=2.0,<3

# Data processing and analysis
numpy>=1.24,<2
pandas>=2.0,<3
scipy>=1.10,<2
scikit-learn>=1.3,<2

# Evaluation and metrics
nltk>=3.8,<4
rouge-score>=0.1,<1
sentence-transformers>=2.2,<4

# Malicious contract classifier (Explainable AI approach)
lightgbm>=4.0,<5
lime>=0.2,<1
shap>=0.42,<1

# Visualization and logging
matplotlib>=3.7,<4
seaborn>=0.12,<1
wandb>=0.15,<1
tensorboard>=2.14,<3

# Hugging Face Hub
huggingface_hub>=0.17,<1

# HTTP requests
requests>=2.28,<3
flask-cors>=4.0,<5

# GPU monitoring
nvidia-ml-py3>=7.0,<8

# File processing and utilities
tqdm>=4.65,<5
click>=8.0,<9
pyyaml>=6.0,<7
jsonlines>=3.1,<5

# Report generation
jinja2>=3.1,<4

# Web framework
flask>=3.0,<4

# Development and testing
pytest>=7.0,<9
black>=23.0,<25
flake8>=6.0,<8
mypy>=1.0,<2

# Optional: Flash Attention 2 for faster inference
# Requires CUDA toolkit; install with: pip install flash-attn --no-build-isolation
# flash-attn

# Optional: For distributed training
# deepspeed>=0.10.0
